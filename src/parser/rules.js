// This file was generated by lezer-generator. You probably shouldn't edit it.
const {LRParser} = require("@lezer/lr")
const {getSymbolChars} = require("./tokens")
exports.parser = LRParser.deserialize({
  version: 13,
  states: "#SQYQROOOOQO'#C_'#C_O_QQO'#C^OOQP'#Cb'#CbQYQROOOdQRO,58xOOQP-E6`-E6`OOQP'#Ca'#CaOOQP'#Cc'#CcOoQRO'#C`OOQO'#Cj'#CjOzQQO1G.dOOQP-E6a-E6aOdQRO,59UOOQP7+$O7+$OOOQO1G.p1G.p",
  stateData: "!S~OZOS[OS~OXPO~O]TO~OXVO_SP`SP~OXVO_SX`SX~O_]O`^O~O",
  goto: "!T_PP`dhnsyPPPPPP!QTROSTQOSQYTR_]VWTX]QSORUSSXT]R[XRZT",
  nodeNames: "âš  rules Rule Head Production Symbol",
  maxTerm: 16,
  skippedNodes: [0],
  repeatNodeCount: 2,
  tokenData: "#d~R^X^}pq}st!r}!O!}!O!P#Y#p#q#_#y#z}$f$g}#BY#BZ}$IS$I_}$I|$JO}$JT$JU}$KV$KW}&FU&FV}~!SYZ~X^}pq}#y#z}$f$g}#BY#BZ}$IS$I_}$I|$JO}$JT$JU}$KV$KW}&FU&FV}~!wQ[~OY!rZ~!r~#QP!`!a#T~#YO]~~#_O`~~#dO_~",
  tokenizers: [getSymbolChars, 0],
  topRules: {"rules":[0,1]},
  tokenPrec: 0
})
