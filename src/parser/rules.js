// This file was generated by lezer-generator. You probably shouldn't edit it.
const {LRParser} = require("@lezer/lr")
exports.parser = LRParser.deserialize({
  version: 13,
  states: "#SQYQPOOOOQO'#C_'#C_O_QPO'#C^OOQO'#Cb'#CbQYQPOOOdQPO,58xOOQO-E6`-E6`OOQO'#Ca'#CaOOQO'#Cc'#CcOoQPO'#C`OOQO'#Cj'#CjOzQPO1G.dOOQO-E6a-E6aOdQPO,59UOOQO7+$O7+$OOOQO1G.p1G.p",
  stateData: "!S~OYOSZOS~O[PO~O]TO~O[VO_SP`SP~O[VO_SX`SX~O_]O`^O~O",
  goto: "!T_PP`dhnsyPPPPPP!QTROSTQOSQYTR_]VWTX]QSORUSSXT]R[XRZT",
  nodeNames: "âš  rules Rule Head Production Symbol",
  maxTerm: 16,
  skippedNodes: [0],
  repeatNodeCount: 2,
  tokenData: "$Q~R`X^!Tpq!Tst!x}!O#T!O!P#`!c!}#e#T#o#e#p#q#{#y#z!T$f$g!T#BY#BZ!T$IS$I_!T$I|$JO!T$JT$JU!T$KV$KW!T&FU&FV!T~!YYY~X^!Tpq!T#y#z!T$f$g!T#BY#BZ!T$IS$I_!T$I|$JO!T$JT$JU!T$KV$KW!T&FU&FV!T~!}QZ~OY!xZ~!x~#WP!`!a#Z~#`O]~~#eO`~~#jR[~wx#s!c!}#e#T#o#e~#xP[~wx#s~$QO_~",
  tokenizers: [0],
  topRules: {"rules":[0,1]},
  tokenPrec: 0
})
